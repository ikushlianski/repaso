# Token and Context Limits

## How AI Models Handle Content

**GPT-3.5-turbo**: Handles up to 4,096 tokens of input and output - perfect for most learning content and card generation.

**GPT-4**: Processes up to 8,192 tokens of input with 4,096 tokens of output - great for complex content analysis and detailed card creation.

**Claude Haiku**: Manages up to 200,000 tokens of input with 4,096 tokens of output - excellent for processing very long documents and comprehensive content.

**Gemini Pro**: Handles up to 32,768 tokens of input with 8,192 tokens of output - ideal for complex analysis and detailed content processing.

## Your Monthly Token Allowance

**Free Tier**: 10,000 tokens per month across all AI models - perfect for trying out AI features and basic content processing.

**Premium Tier**: 100,000 tokens per month with priority processing - handles regular AI-assisted learning and content creation.

**Pro Tier**: 500,000 tokens per month with advanced model access - supports intensive AI usage for professional learning projects.

## Document Processing Capabilities

**Free Tier**: Documents up to 5,000 tokens (roughly 3,750 words) - great for articles, short papers, and focused content.

**Premium Tier**: Documents up to 20,000 tokens (roughly 15,000 words) - handles longer articles, chapters, and comprehensive materials.

**Pro Tier**: Documents up to 100,000 tokens (roughly 75,000 words) - supports very long documents, books, and extensive content libraries.

## Smart Content Processing

**Intelligent Chunking**: We automatically split large documents into manageable pieces while preserving the overall meaning and context.

**Context Preservation**: We maintain understanding across document chunks so the AI can create coherent and relevant flashcards.

**Memory Management**: We clear context when switching between different documents to ensure each piece of content gets focused attention.

**Efficient Processing**: We use optimized prompts and processing techniques to get the most value from your token usage.

## Cost Management

**Usage Tracking**: We monitor your token usage across all AI models and request types to help you understand your consumption patterns.

**Smart Model Selection**: We automatically route your content to the most appropriate AI model based on complexity and requirements.

**Response Caching**: We cache similar responses to reduce repeated token usage and improve efficiency.

**Batch Processing**: We group similar requests together to optimize token efficiency and reduce costs.
